{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from include.RandomHelper import check_data_state\n",
    "check_data_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {return false;}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first part of the task, the data sets that were provided had to be examined and reduced in such a way that only a certain phase space remained - containing events that have a signal-like signature - a decay into four leptons via a Z boson pair, where one Z boson is virtual.\n",
    "\n",
    "As a final step, a very rough estimate of the significance was made ($Z \\approx s/\\sqrt{b}$) and it was discussed why this is unsuitable.\n",
    "\n",
    "In this part of the task two methods are presented how the significance of a measurement can be determined. The first method (local $p_0$ value) was used in the discovery of the Higgs boson - a simplified version of this method is presented in this section. The second method determines the significance using the signal probability.\n",
    "\n",
    "In both methods, we will not consider the question of systematic uncertainty, since the addition of higher-dimensional fits has to be made, which are not necessarily more complicated, but much more time-consuming.\n",
    "\n",
    "You can continue to use the obtained data sets from part one here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization of the background\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid fluctuations it is advantageous to express the background distribution of the MC simulations in the first step by a probability density.\n",
    "\n",
    "The difference between a fit and a parameterization should again be clarified: The application of an fit always occurs when a certain theory previously established for it is to be confirmed or falsified by the measured data. The data contained for the fitting have uncertainty, which depends on the experiment and the measurement methods, but is fixed mainly by the measurement of the data.\n",
    "\n",
    "In the case of a parameterization, the aim is to express the existing measurement, which in this example is available in the form of a histogram, by a different form. In this case it should be a continuous function - more precisely: a continuous probability density function (pdf) in a certain interval.\n",
    "\n",
    "Thus, the main difference between an adaptation and a parameterization is clear: While in the first case a dependence given by the theory is present, in the second case an attempt is made to find a parameterization that describes the data in another form as accurately as possible.\n",
    "\n",
    "For this reason - and especially in this case - it is not possible to find an exact function for it.Nevertheless to be able to make a selection and to justify it, the procedure after $\\chi^2$ can be used: For a $\\chi^2$ fit the uncertainties on the data are decisive whether a suitable model can be chosen for the data ($\\frac{\\chi^2}{n_{\\mathrm{df}}}$) or not. Conversely, the uncertainties on the data can be scaled until a model fits. The question, which model parameterizes the data better, can now be answered with the scaling factor.\n",
    "\n",
    "However, not only the smallest scaling factor is the decisive point, since this can always be realized by a polynomial of a very high degree. The so-called overfitting should be avoided, because only the data behaviour should be parameterized and not the fluctuation observed in this measurement. The choice should therefore be made for a function which still contains a sufficiently small number of parameters, but already keeps the scaling factor for the uncertainty as small as possible.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Use a suitable model to parameterize the background. Also make sure that the model you choose is the most suitable one.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of linear combinations of the legendre polynomials has already been carried out and summarized in the class `LLC`. Independent functions can also be used (see the numpy documentation).\n",
    "\n",
    "The use of linear combinations of legendre polynomials offers the advantage over normal polynomials that the legendre polynomials form an orthogonal function system and behave robustly in case of an fit - unlike normal polynomials. The correlation of the uncertainties on the individual parameters is reduced due to the orthogonality, whereas with normal polynomials there is almost always a strong correlation between the individual parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.parameterization.LegendrePoly import LegendreLinearCombination as LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llc_background = LLC(x_mean=0.0) # centering around x_mean\n",
    "\n",
    "x_ = np.linspace(-1, 1, 1000)\n",
    "plt.plot(x_, llc_background.grade_0(x_, a=1))\n",
    "plt.plot(x_, llc_background.grade_1(x_, a=1, b=1))\n",
    "plt.plot(x_, llc_background.grade_2(x_, a=1, b=1, c=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation, the number of bin numbers and the considered interval should be selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\"  \n",
    "llc_background = LLC(x_mean=(999.1 + 999.2)/2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>\n",
    "\n",
    "The uncertainties thus calculated are fundamentally different from the $\\sqrt{N}$ variant - they describe the scaled fluctuation of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"background\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.fits.McFitInit import McFitInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir, info=info)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of the appropriate signal MC\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to narrow down the possible signal MC simulations and to decide on a single one and continue working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary parameterization of signal MC\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different signal MC simulations are available. These were roughly parameterized by a Gaussian distribution $\\mathcal{G} \\left( m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G} \\right)$. The aim is to determine the dependence of the width on the invariant mass and to describe it using a suitable model. The resolution of the detector's transversal pulse is the biggest influence.\n",
    "\n",
    "The results of these parameterizations are summarized in a csv file:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of these Gaussian parameterizations are summarized in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ = pd.read_csv(\"../data/for_long_analysis/other_mc_gauss_param/gauss_params.csv\")\n",
    "df_\n",
    "sigma, sigma_err, mu, mu_err = tuple(df_[[\"sigma\", \"sigma_err\", \"mu\", \"mu_err\"]].values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize $\\sigma_{\\mathrm{G}}(m_{4\\ell})$ using the existing data.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_model(x, *args):\n",
    "    \"\"\"\n",
    "    Model to for sigma parameterization\n",
    "    \n",
    "    \"\"\"\n",
    "    # code\n",
    "    \n",
    "    return x\n",
    "\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This variant - the continuous parameterization of the signal MC does not happen in the correct analysis and is used here mainly for time reasons. Normally, a separate MC simulation is created for each Higgs mass, with which all further steps are performed. However, creating such a simulation is extremely time-consuming. A linear interpolation of a Gaussian-like parameterization, however, already leads to a sufficient result, that we want to achieve in the context of this section: The justification of the choice of a certain signal MC simulation based on the measurement - as well as an estimation of the statistical significance (more about this in the next sections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood-ratio test\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood ratio test is explicitly used for the selection between the different simulations.\n",
    "For this purpose, the null hypothesis $H_0$, which only corresponds to the background, and the counter hypothesis $H_1$, which corresponds to the background and a signal, are compared with each other.\n",
    "\n",
    "The extended unbinned likelihood of the $H_0$ hypothesis is based on the parameterization of the background $f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}})$ where $f_{\\mathrm{U}}$ is the previously determined model from the background parameterization with the optimal parameters $\\theta_{\\mathrm{U}}$.\n",
    "\n",
    "The distribution used for the alternative hypothesis can be described by $$f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) = \\mu f_{\\mathrm{S}}(m_{4\\ell}|\\theta_{\\mathrm{S}}) + f_{\\mathrm{U}}(m_{4\\ell}|\\theta_{\\mathrm{U}}) \\, , \\\\ \\int_{\\Omega} f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) \\mathrm{d}m_{4\\ell} = 1 $$ $f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}})$ is a Gaussian distribution $\\mathcal{G}(m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G}}(m_{\\mathrm{H}} ))$ with fixed values for the mass and width. For the estimator $\\mu$ the limitation applies: $$\\mu \\geq 0 \\, .$$ This restriction leads to a negligible distortion and is introduced for physical considerations, since in the distribution of invariant masses a excess of events is expected if a Higgs boson is present.\n",
    "\n",
    "Thus, the quantity to be considered - the ratio of the likelihoods of both hypotheses - can be formed. For numerical stability, the negative logarithm of this is formed. The factor two is necessary for a later used asymptotic form.\n",
    "\n",
    "$$ q_0 = -2 \\ln \\left( \\frac{\\mathcal{L}(x|\\mu = \\hat{\\mu})}{\\mathcal{L}(x|\\mu = 0)} \\right) \\, .$$ \n",
    "$\\hat{\\mu}$ here is the ideal estimator for a fixed mass and width of the signal hypothesis.\n",
    "\n",
    "The test statistic $q_0$ used here is a simplified version of the test statistic used at the LHC for this analysis. It corresponds to the test statistics used at the LEP and has the disadvantage of the bias, that is already mentioned. However, the advantage of the test statistic is that it is easier to implement and the calculation takes less time because no Toy Monte Carlo simulations are necessary. Likewise, even for a small number of events the analytical form can be used, which the test statistic takes in the borderline case of many events.\n",
    "\n",
    "Remember: The extended likelihood function is defined as follows: $$ \\mathcal{L}(x|\\mu) = \\frac{\\nu(\\mu)^N}{N!}\\mathrm{e}^{-\\nu(\\mu)}\\prod_i^N f_{\\mu\\mathrm{S+B}}(x_i|\\mu) $$ and differs in the scaling factor from the normal likelihood function. Due to this scaling factor the number of measurements (since it is a counting experiment) plays a certain role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = hist_range\n",
    "mass_array = np.linspace(lower_bound, upper_bound, 100)\n",
    "sigma_model_arguments = tuple([])\n",
    "sigma_array = sigma_model(mass_array, *sigma_model_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement the likelihood ratio test. For numerical integration, you can use the ['scipy.integrate.quad'](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html) method. For minimizing the likelihood function you can use ['iminuit'](https://nbviewer.jupyter.org/github/scikit-hep/iminuit/blob/master/tutorial/basic_tutorial.ipynb)\n",
    ").\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 9999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\" \n",
    "ru_dir = \"../data/for_long_analysis/ru_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "data = raw_data.data\n",
    "data\n",
    "\n",
    "# scale factors for the mc simulations\n",
    "b_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "_, _, background_hist, _ = b_mc_d.variables_for_hist_fit\n",
    "scale_b_mc = np.sum(background_hist)\n",
    "\n",
    "s_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"signal\", mc_dir=mc_dir)\n",
    "_, _, signal_hist, _ = s_mc_d.variables_for_hist_fit\n",
    "scale_s_mc = np.sum(signal_hist)\n",
    "\n",
    "scale_s_mc, scale_b_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iminuit Example:\n",
    "from iminuit import Minuit\n",
    "\n",
    "def extended_likelihood(mu=0.0):\n",
    "    # FUnction that need to be minimized\n",
    "    #code\n",
    "    return mu\n",
    "\n",
    "m = Minuit(extended_likelihood, \n",
    "           mu=0.5, error_mu=0.1, limit_mu=(0.0, 3.0),\n",
    "           errordef=0.5)\n",
    "m.migrad()\n",
    "best_mu = m.values[\"mu\"]\n",
    "\n",
    "\n",
    "q0 = np.zeros(len(mass_array))\n",
    "\n",
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymptotic form of the probability density of $q_0$ when no signal is expected is described by\n",
    "$$ f(q_0|\\hat{\\mu}=0) \\rightarrow \\frac{1}{2}\\left( \\delta (q_0) + \\chi^2_{1} \\right) \\, .$$\n",
    "from this the $p_0$ value can be calculated according to:\n",
    "$$ p_0 = \\int\\limits_{q_{0_{\\mathrm{obs}}}}^{\\infty} \\mathrm{d}q_0 f(q_0|\\hat{\\mu}=0) \\, . $$\n",
    "using this, the significance can be estimated as follows: $$Z = \\Phi^{-1}(1-p_0) = \\sqrt{q_{0_{\\mathrm{obs}}}} \\, .$$ However, in this case it should only be considered as a first estimation because the parameterization of the input signal is not accurate.\n",
    "Likewise, the systematics are not considered at this point, only the statistical uncertainty is included in the calculation of $p_0$ or $Z$.\n",
    "\n",
    "In 2012, the particle excess and the assignment of this excess to a Higgs boson which then led to its discovery was carried out using this method. \n",
    "\n",
    "Furthermore, another method for determining significance is presented, which - although with slightly different assumptions - leads to a similar significance as in the last part of the task and provides a good possibility for a cross-check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is useful to save the temporary result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0, p0 = np.zeros(len(sigma_array)), np.zeros(len(sigma_array))\n",
    "df = pd.DataFrame(data=np.array([mass_array, sigma_array, q0, p0, np.sqrt(q0)]).T,\n",
    "                  columns=[\"mass_array\", \"gauss_sigma\", \"q0\", \"p0\", \"sqrt_q0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Visualize the results. For the significance levels the 'get_p0_sigma_lines' from 'PlotHelper' can be used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.stattest.PlotHelper import PlotHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.get_p0_sigma_lines(x_=mass_array, \n",
    "                             ax_obj_=ax, \n",
    "                             max_sigma_=4)\n",
    "ax.plot(df.mass_array, df.p0+1)  # <- change this line accordingly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examination of the final distributions\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is a repetition and is available in the first section - but offers a good possibility to visualize the initial situation again - with the only difference that there is now a reason why exactly the 125 GeV Signal MC Simulation was used.\n",
    "\n",
    "\n",
    "Other quantities can still be displayed (although it is questionable whether 'z1_index', 'z2_index', as well as 'z1_tag' or 'z2_tag' are quantities that require detailed visual consideration):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.ApplyHelper import ProcessHelper\n",
    "print(ProcessHelper.print_possible_variables(\"../data/for_long_analysis/mc_aftH/MC_2012_H_to_ZZ_to_4L_2el2mu_aftH.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Examine the distribution of the four lepton invariant masses and explain the $p_0$ value, which depends on the four lepton masses, in relation to the measured events. Why is there no change in the $p_0$ value for larger masses (when the mass interval of $[105, 151]$ is chosen)?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.histogramm.HistOf import HistOf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "h = HistOf(mc_dir=\"../data/for_long_analysis/mc_aftH\",\n",
    "           ru_dir=\"../data/for_long_analysis/ru_aftH\", info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "h.variable(\"energy\", 50, (0, 200))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(r\"$p_T$ in GeV\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The uncertainties presented here are asymmetrical. For this purpose, a Poisson distribution (measured number of events) is determined from the expected value, the lower limit ( - 34%) and the upper limit (+34%), so that a 68% uncertainty interval can be specified. The asymmetry of the Poisson distribution is clearly visible for a small expected value. Only in the limit case of large expected value does the Poisson distribution transition to the Gaussian distribution and the uncertainties on a measured value become symmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization of the signal MC\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the best suited signal hypothesis, the distribution will be parameterized more precisely. This is done analogously to the background parameterization.\n",
    "\n",
    "The motivation is analogous to that given in the section on background parameterization. Only the function forms used for parameterization change: These are bell-shaped in this case.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize the signal distribution with a suitable model. Also make sure that the model you choose is the most suitable one.\n",
    "\n",
    "</div>\n",
    "\n",
    "The students can choose from distributions that have already been implemented:\n",
    "- Gauss distribution\n",
    "- Cauchy distribution\n",
    "- Voigt Profile\n",
    "- Single - Sided - Crystal - Ball\n",
    "- Double - Sided - Crystal - Ball\n",
    "\n",
    "\n",
    "However, independently implemented distributions can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.parameterization.SignalFunction import SignalFunction as SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"signal\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_H_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization \n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Determination of statistical significance\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After successful parameterization of the background and the signal, the significance of the signal can be determined. The following parameterization is selected for this purpose:\n",
    "$$ f(x, \\alpha_\\mathrm{S}) = \\alpha_\\mathrm{S} f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}}) + (1 - \\alpha_\\mathrm{S}) f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}}) \\, .$$\n",
    "\n",
    "The optimal parameter $\\alpha_{\\mathrm{S}}$ corresponds to the signal probability of this measurement. This determination is carried out with the help of the $-2\\ln\\mathcal{L(\\alpha_{\\mathrm{S}})}$ function which is to be implemented independently. The previously determined parameters $\\theta_{\\mathrm{S}}$ and $\\theta_{\\mathrm{U}}$ of the probability density function of the signal or background are fixed for the determination (profiled Likelihood).\n",
    "\n",
    "The evaluation of the function value $-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)$ should be emphasized here. This corresponds to the assumption that there is no signal component in the existing measurement and the measurement can therefore be explained by the null hypothesis (only background). In asymptotic form, therefore, the significance can be determined analogously to the expression in the chapter on parameterization of the signal MC expression with: $$ Z = \\sqrt{\\frac{-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)}{\\mathcal{L}_{\\mathrm{min}}}} \\, .$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "Implement the $-2\\ln\\mathcal{L(\\alpha_S, m_{4\\ell})}$ function (if not already present).Determine the signal probability plus uncertainty of the measurement.  Specify the significance with which a higgs-like boson is measured. In addition, vary the mass of the higgs-like boson for the two-dimensional variant and compare it with the literature. Visualize and discuss your results.\n",
    "    \n",
    "For the minimization you can use `minuit` as in QRT. The numerical integration can again be done using `scipy.integrate.quad`.\n",
    "    \n",
    "It is also possible to perform an `UninnedFit` using `kafe2` and implement the extended likelihood as a cost function independently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# measurement:\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\" \n",
    "ru_dir = \"../data/for_long_analysis/ru_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "raw_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code iminuit\n",
    "\n",
    "def extended_nll_iminuit(alpha_s, mass=125.0):\n",
    "    # code\n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code kafe2\n",
    "\n",
    "def extended_nll_kafe2(data, model, parameter_values, parameter_constraints):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: measurement array\n",
    "    :param model: model array (calculate individual in # code)\n",
    "    :param parameter_values:\n",
    "    :param parameter_constraints: (not used)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if one of the parameter_values is nan\n",
    "    if any(f\"{par}\" == \"nan\" for par in parameter_values):\n",
    "        return np.inf\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    _total_log_likelihood = 0.0 # your code\n",
    "    \n",
    "    # check if _total_log_likelihood is nan\n",
    "    if np.isnan(_total_log_likelihood):\n",
    "        return np.inf\n",
    "    return -2.0 * _total_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1D profiled Likelihood\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The one-dimensional variant only varies the signal probability. The mass is held after the parameterization and is not varied anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code (iminuit minimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.stattest.PlotHelper import PlotHelper\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha_ = np.linspace(0.0, 0.8, 1000)\n",
    "nll_ = extended_nll(alpha_s=alpha_, mass=125)\n",
    "\n",
    "# as example (to be replaced with actual nll array)\n",
    "nll_ = 150*(alpha_ - 0.4) ** 2\n",
    "\n",
    "z_ = np.sqrt(nll_[0])\n",
    "\n",
    "# code to further customize the plot\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.plot_nll_scan_main_window(ax_obj_=ax, \n",
    "                                     x_=alpha_, \n",
    "                                     y_=nll_, \n",
    "                                     z_=z_)\n",
    "\n",
    "ax_in = inset_axes(ax, width='50%', height='40%', loc=\"upper right\")\n",
    "PlotHelper.plot_nll_scan_inside_window(ax_obj_=ax_in, \n",
    "                                       x_=alpha_, y_=nll_, \n",
    "                                       x_ticks_=PlotHelper.get_error_points_from_nll(x_=alpha_, nll_=nll_))\n",
    "mark_inset(ax, ax_in, loc1=2, loc2=4, fc=\"none\", ec=\"grey\", lw=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the 'kafe2' variant, an 'uninned container' must be created and adjusted. For the 1D variant it is recommended to use the method `fix_parameter(par_name, value)`.\n",
    "The significance can be calculated using the implemented `extended_nll_kafe2` cost function. It is recommended that you use the `limit_parameter` to restrict the parameter `alpha` to a reasonable interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import kafe2 as K2\n",
    "\n",
    "my_cost_function = K2.UnbinnedCostFunction_UserDefined(extended_nll_kafe2)\n",
    "\n",
    "# more code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2D profiled Likelihood - Mass determination\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, the mass is also determined. This is done by the two-dimensional variation - mass and signal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similar to the 1D version, the best mass can be determined at the same time. The procedure is analogous to the 1D variant/QRT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code:\n",
    "#     - minimization\n",
    "#     - plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The only difference to the 1D variant is that the mass is no longer fixed by `fix_parameter`, but is varied in a certain interval (`limit_parameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Combination of the measurements: CMS and ATLAS\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to obtain a better statistical significance, it is advantageous to include further measurements in the overall assessment. This can be done either by adding more data or by another experiment. Within the scope of this task the second option is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the ATLAS experiment, therefore, similar steps as above would have to be carried out, although many of the cuts show deviations. To shorten the work a bit, the final measurement is provided, as well as the histogram entries of the MC simulations.\n",
    "\n",
    "In addition, the parameters of the most suitable functions (linear combination of centered legend trepolynomials up to the third order for the background and the DSCB distribution for the signal) are provided (but it is also possible to check these results yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kafe2 as K2\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "atlas_data_dir_ = \"../data/for_long_analysis/atlas_data_and_mc_hist/\"\n",
    "atlas_files = [os.path.join(atlas_data_dir_, file) for file in os.listdir(atlas_data_dir_) if \".csv\" in file and \"atlas\" in file]\n",
    "pprint(atlas_files)\n",
    "hist_range_ = (106, 151)\n",
    "num_bins = 45\n",
    "\n",
    "data_A = np.loadtxt(atlas_files[2], delimiter=\",\")\n",
    "data_A = data[(data >= hist_range_[0]) & (data <= hist_range_[1])]\n",
    "\n",
    "mc_bac = np.loadtxt(atlas_files[1], delimiter=\",\").T\n",
    "mc_bac_data = mc_bac[0]\n",
    "mc_bac_error = mc_bac[1]\n",
    "\n",
    "mc_sig = np.loadtxt(atlas_files[0], delimiter=\",\").T\n",
    "mc_sig_data = mc_sig[0]\n",
    "mc_sig_error = mc_sig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_CMS = data\n",
    "data_ATLAS = data_A\n",
    "\n",
    "background_parameter_CMS = background_parameter\n",
    "signal_parameter_CMS = signal_parameter\n",
    "\n",
    "background_parameter_ATLAS  = np.array([0.025050083639971264, \n",
    "                                        0.00018744120591615378, \n",
    "                                        -1.178181555551107e-05, \n",
    "                                        1.1872474265844408e-07])\n",
    "signal_parameter_ATLAS = np.array([2.1395802886491615, \n",
    "                                   124.19274632605674, \n",
    "                                   0.8324353801486924, \n",
    "                                   1.8034218165656613, \n",
    "                                   14.109501070824912, \n",
    "                                   19.9999999999894])\n",
    "bfu_ATLAS = llc_background.grade_3\n",
    "sfu_ATLAS = sf.DSCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Combine the two measurements and determine the resulting mass of the Higgs boson, which is the signal probability and the significance of a measurement of the Higgs boson.\n",
    "\n",
    "You can reuse/modify the functions you have already implemented.\n",
    "    \n",
    "Note:\n",
    "    $$ \\mathcal{L_{\\mathrm{tot}}(\\alpha_S, m_{4\\ell})} = \\mathcal{L_{1}(\\alpha_S, m_{4\\ell})} \\mathcal{L_{2}(\\alpha_S, m_{4\\ell})} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code (1D and 2D minimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
