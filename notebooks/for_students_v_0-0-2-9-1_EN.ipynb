{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 0.0.2.9.1\n",
    "\n",
    "TODO:\n",
    "  * add collection of .ig files, WW, GammaGamma events included\n",
    "  * Add collection of .ig files that do not contain events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {return false;}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, partly covered by a colloquium, the origin of the data sets used in the following will be examined in more detail: the CMS detector. The construction of the detector and the functionality of individual detector components will be discussed. In addition, events are to be analyzed and important physical quantities should be displayed visually. For both cases the web interface [Ispy-WebGL](https://github.com/cms-outreach/ispy-webgl) can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Identify and name all decays from the local collection and save the representations of some of them.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an Internet connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://ispy-webgl.web.cern.ch/ispy-webgl/\" width=\"100%\" height=\"700\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without an internet connection: Open the `index.html` from the [Github Repository](https://github.com/cms-outreach/ispy-webgl) locally in a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, only events from the decay into four leptons (muons and electrons, not tau leptons) are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data format\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format used in the following, from which all necessary sizes are taken, is a modified .CSV format. The advantage of this format is that the data can be viewed at any time in the same way as a human would read it.\n",
    "\n",
    "This section serves to familiarize with the data format in order to be able to perform the task in the following section.\n",
    "\n",
    "The separation of individual variables in an event is done using \";\". The entries of the individual leptons within an event, on the other hand, are classically separated with \",\". This gives the advantage of individual numbers of leptons in an event without the introduction of additional placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "name_1 = \"data/mc_init/MC_2012_ZZ_to_4L_2el2mu_init.csv\"\n",
    "name_2 = \"data/mc_init/MC_2012_ZZ_to_4L_4mu_init.csv\"\n",
    "\n",
    "dataframe_1 = pd.read_csv(name_1, delimiter=\";\")\n",
    "dataframe_2 = pd.read_csv(name_2, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For processing individual sizes, the respective elements from the string (`str`) data format can be converted back into a list (`list`):\n",
    "For this purpose the options are available either the standard library `ast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "px = ast.literal_eval(f\"[{dataframe_2.loc[3, 'px']}]\")\n",
    "px = np.array(px, dtype=float)\n",
    "px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or to use the `split` method for the `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = dataframe_2.loc[3, 'py'].split(\",\")\n",
    "py = np.array(py, dtype=float)\n",
    "py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, new quantities such as the transverse momentum can be determined, on the basis of which a limitation in the quantities can and should be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = np.sqrt(px ** 2 + py ** 2)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For muons, a minimum value of 5 GeV for the transverse impulse must be exceeded. For transverse impulse values below this, the probability of muon misidentification rises.\n",
    "In this respect, all muons in this event that do not meet the condition must be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_minimum_filter = pt > 5\n",
    "pt_minimum_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt = pt[pt_minimum_filter]\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the filters by using custom Apply Class\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filter is applied to all variables within an event and is different from the filter for the next event.\n",
    "The application of the filter to the data set in series is already implemented and can be applied via the Apply class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.Apply import Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Apply object is created, an \"allowed\" and a \"calculation\" class instance must also be passed. The required variables or filters for the application of the filters are calculated using these classes.\n",
    "For the example of the minimum transverse impulse the two classes can be represented as a collection of functions, which could stand alone (`@staticmethod`), but logically combined in one (two) class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calc_Start(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def pt(px, py):\n",
    "        return np.sqrt(px ** 2 + py ** 2)\n",
    "\n",
    "class Allowed_Start(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def min_pt(pt, look_for):\n",
    "        return pt > 7.0 if look_for == \"electron\" else (\n",
    "            pt > 5.0 if look_for == \"muon\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_2)\n",
    "process = Apply(input_=name_2, particle_type=\"muon\", use_n_rows=10000,\n",
    "                calc_instance=Calc_Start, allowed_instance=Allowed_Start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data set `name_2` the variable of the transverse pulse does not yet exist. In an intermediate step, which is already contained in the filter `\"check_min_pt\"`, the transverse pulse can also be added explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.add_variable(\"pt\")\n",
    "process.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of such a variable can be displayed by the method `hist_of_variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.hist_of_variable(variable=\"pt\", bins=100, hist_range=(0, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the filter for the minimum transverse impulse and removing all subsequent events containing less than four leptons provides the appropriate distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.filter(filter_name=\"check_min_pt\")\n",
    "process.hist_of_variable(variable=\"pt\", bins=100, hist_range=(0, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other variables can also be visualized by the histogram.\n",
    "The performed cut mainly influences the distribution of the transverse impulse, but changes little in the distribution of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Examine the distributions of some of the quantities present in the records.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T11:48:56.790504Z",
     "start_time": "2020-05-21T11:48:56.785410Z"
    }
   },
   "outputs": [],
   "source": [
    "# here goes the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculation of important variables\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Following the example of the transversal impulse, implement all subsequently necessary quantities and visualize them appropriately:\n",
    "  * Pseudorapidity $\\eta$\n",
    "  * Transverse pulse $p_T$\n",
    "  * Azimuthal angle $\\phi$\n",
    "\n",
    "And explain your observations.\n",
    "\n",
    "(Check their implementations using MC simulations of the background)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class skeleton used for this inherits the previous method for the transverse momentum and an initial class containing methods such as the reconstruction of Z-boson pairs are added afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class CalcStudent(Calc_Start):\n",
    "    '''\n",
    "    Class for the calculation of certain sizes that are used for\n",
    "    the cuts or are essential for the reconstruction.\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def combined_charge(charge, combine_num):\n",
    "        '''\n",
    "        Tests whether an electrically neutral charge combination is possible.\n",
    "\n",
    "        :param charge: ndarray\n",
    "                       1D array containing data with \"int\" type.\n",
    "        :param combine_num: int\n",
    "                            4 if look_for is not \"both\", 2 else\n",
    "        :return: bool\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def eta(px=None, py=None, pz=None, energy=None):\n",
    "        '''\n",
    "        Calculates the pseudorapidity.\n",
    "        Optional with or without energy.\n",
    "\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param pz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param energy: ndarray\n",
    "                       1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def invariant_mass_square(px, py, pz, energy=None, eta=None, phi=None):\n",
    "        '''\n",
    "        Calculates the square of the invariant mass.\n",
    "        Optional with or without energy.\n",
    "        Optionally with or without eta and phi.\n",
    "\n",
    "        :param phi: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param pz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param energy: ndarray\n",
    "                       1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def phi(px, py):\n",
    "        '''\n",
    "        Calculation of the angle phi.\n",
    "\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py:  ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_phi(phi1, phi2):\n",
    "        '''\n",
    "        Calculation of the difference between two phi angles.\n",
    "\n",
    "        :param phi1: ndarray\n",
    "                     1D array containing data with \"float\" type.\n",
    "        :param phi2: ndarray\n",
    "                     1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_r(eta, phi):\n",
    "        '''\n",
    "        Calculation of delta_r.\n",
    "\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param phi: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the cuts\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the cut to the minimum transverse pulse, further cuts can be introduced which are then used as well.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement the methods listed in the class `AllowedStudent`. Estimate the choice of cuts using the distributions in the chapter **Application of the filters by using custom Apply Class**, the **detector** and the **kinematic constraints** of the events.\n",
    "\n",
    "(Check your implementations using the MC simulations of the background).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class AllowedStudent(Allowed_Start):\n",
    "    '''\n",
    "    Class that introduces certain cuts and thus restricts the leptons in the events.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta_r(delta_r):\n",
    "        '''\n",
    "        Checks if delta_r is smaller than the allowed value.\n",
    "\n",
    "        :param delta_r: ndarray\n",
    "                        1D array containing data with `float` type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def rel_pf_iso(rel_pf_iso):\n",
    "        '''\n",
    "        Checks if rel_pf_iso is smaller than the allowed value.\n",
    "\n",
    "        :param rel_pf_iso: ndarray\n",
    "                           1D array containing data with `float` type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def misshits(misshits):\n",
    "        '''\n",
    "        Checks if the minimum number of misshits was kept.\n",
    "\n",
    "        :param misshits:\n",
    "        :return:\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def pt(p_t, look_for, coll_size=4):\n",
    "        '''\n",
    "        Checks if the exact pedingun regarding pt is observed.\n",
    "        (>20 GeV: >= 1; >10 GeV: >= 2; >Minimum pt: >= 4).\n",
    "\n",
    "        :param p_t: ndarray\n",
    "                    1D array containing data with `float` type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :param coll_size: int\n",
    "                          4 if look_for is not \"both\", 2 else\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def eta(eta, look_for):\n",
    "        '''\n",
    "        Checks if the pseudorapidity of leptons is valid.\n",
    "\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def lepton_type(typ, look_for):\n",
    "        '''\n",
    "        Checks for the permitted classification of leptons.\n",
    "\n",
    "        :param typ: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def impact_param(sip3d, dxy, dz):\n",
    "        '''\n",
    "        Checks if the impact parameters of the collision are valid and sorts out\n",
    "        events that do not have a clear and equal collision point.\n",
    "\n",
    "        :param sip3d: ndarray\n",
    "                      1D array containing data with \"float\" type.\n",
    "        :param dxy: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param dz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def zz(z1, z2):\n",
    "        '''\n",
    "        Checks if the Z1 candidate and the Z2 candidate is within the allowed range.\n",
    "\n",
    "        :param z1: float\n",
    "        :param z2: float\n",
    "        :return: bool\n",
    "        '''\n",
    "        # code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of the implemented code with the partially provided classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.CalcAndAllowerInit import AllowedInit\n",
    "from include.processing.CalcAndAllowerInit import CalcInit\n",
    "\n",
    "AllowedInit.a_allowed_instance = AllowedStudent\n",
    "AllowedInit.a_calc_instancea = CalcStudent\n",
    "class Allowed(AllowedStudent, AllowedInit):\n",
    "    pass\n",
    "    \n",
    "CalcInit.c_allowed_instance = Allowed\n",
    "CalcInit.c_calc_instance = CalcStudent\n",
    "class Calc(CalcStudent, CalcInit):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of filters and reconstruction on MC - simulations\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing implementations of the cuts should be tested using the background MC simulations. This is to prevent the targeted work towards a certain goal (actual measurement), since a small number of later events in the actual measurement should avoid the fact of subjectively selecting specific events in an area as far as possible.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Now apply all the cuts you have implemented to the MC simulations and measurements.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Therefore it is useful to create a list of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.ApplyHelper import ProcessHelper\n",
    "\n",
    "\n",
    "# All Background MC and signal MC for m_H = 125 GeV\n",
    "mc_files = not True\n",
    "# Signal MC for other masses m = [115, 120, 122, 124, 128, 130, 135, 140, 145, 150]\n",
    "other_mc_files = not True\n",
    "# actual measurement\n",
    "run_files = True\n",
    "\n",
    "# folder with initial records: measurement\n",
    "dir_measurement = \"data/ru_aftT/\"\n",
    "# folder with initial records: backround MC and singal (125 GeV) MC\n",
    "dir_mc = \"data/mc_aftT/\"\n",
    "# folder with initial records: other masses\n",
    "dir_other_mc = \"data/other_mc/\"\n",
    "\n",
    "file_tuples = []\n",
    "if mc_files:\n",
    "    file_tuples += ProcessHelper.create_tuple(dir_mc)    \n",
    "if other_mc_files:\n",
    "    import os\n",
    "    list_of_other_mc = []\n",
    "    for name in os.listdir(dir_other_mc):\n",
    "        file_tuples += ProcessHelper.create_tuple(os.path.join(dir_other_mc, name, \"mc_aftT/\"))\n",
    "if run_files:\n",
    "    file_tuples += ProcessHelper.create_tuple(dir_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single such `namedtuple` contains the file and particle type of the data set (necessary for `Apply`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tuples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole filter and reconstruction process can be combined in one function. The sequence of the filters is critical for the runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following filter and reconstruction steps are available for selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can have the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_reco_process(used_pair):\n",
    "    process = Apply(input_=used_pair.name, \n",
    "                    particle_type=used_pair.particle, \n",
    "                    multi_cpu=True, use_swifter=False,\n",
    "                    calc_instance=Calc, \n",
    "                    allowed_instance=Allowed)\n",
    "    \n",
    "    # Logische Reihenfolge Wählen\n",
    "    # quicksave: Speichert den Datensatz NACH dem Anwenden \n",
    "    # des Filter- bzw Rekonstruktionsschrittes\n",
    "    \n",
    "    process.filter(filter_name=\"check_type\",\n",
    "                   quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftT\"))\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    process.filter(filter_name=\"check_min_pt\")\n",
    "    process.filter(filter_name=\"check_impact_param\")\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    process.filter(filter_name=\"check_exact_pt\")\n",
    "    process.filter(filter_name=\"check_m_2l\")\n",
    "    process.filter(filter_name=\"check_rel_iso\")\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    if process.particle_type != \"muon\":\n",
    "        process.filter(filter_name=\"check_misshit\")\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    process.filter(filter_name=\"check_eta\")\n",
    "    process.filter(filter_name=\"check_q\")\n",
    "    process.filter(filter_name=\"check_m_4l\",\n",
    "                   quicksave=ProcessHelper.change_on_affix(used_pair.name, \"befZ\"))\n",
    "    process.reconstruct(reco_name=\"zz\", \n",
    "                        quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftZ\"))\n",
    "    process.reconstruct(reco_name=\"mass_4l_out_zz\",\n",
    "                        quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftH\"))\n",
    "    del process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Select the appropriate sequence of filter and reconstruction steps.\n",
    "    \n",
    "(To estimate the speed with which the individual filters are executed on a data set, it is recommended to use the data set of the background MC simulations and use the following 'kwargs' when initializing the `Apply` instance: `multi_cpu=False, use_swifter=True`)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of `filter_and_reco_process` to the records: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "for pair in tqdm(file_tuples):\n",
    "    filter_and_reco_process(pair)\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization of the background\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid fluctuations, it is advantageous to express the background distribution of the MC simulations by a probability density function (pdf).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Use a suitable model to parameterize the background. Also make sure that the model you choose is the most suitable one.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation of linear combinations of the legend trepolynomials has already been carried out and summarized in the class `LLC`. Independent functions can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.parameterization.LegendrePoly import LegendreLinearCombination as LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llc_background = LLC(x_mean=0.0) # centering around x_mean\n",
    "\n",
    "x_ = np.linspace(-1, 1, 1000)\n",
    "plt.plot(x_, llc_background.grade_0(x_, a=1))\n",
    "plt.plot(x_, llc_background.grade_1(x_, a=1, b=1))\n",
    "plt.plot(x_, llc_background.grade_2(x_, a=1, b=1, c=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation, the number of bin numbers and the considered interval should be selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, hist_range, info = 9999, (9999, 9999), [[\"2012\"], [\"A-D\"]]\n",
    "mc_dir = \"./data/mc_aftH/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"background\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.fits.McFitInit import McFitInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection of the appropriate signal MC\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this section is to narrow down the possible signal MC simulations and to decide on a single one and continue working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary parameterization of signal MC\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different signal MC simulations are available. These were roughly parameterized by a Gaussian distribution $\\mathcal{G} \\left( m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G}} \\right)$. The aim is to determine the dependence of the width on the invariant mass and to describe it by a suitable model. The resolution of the detector's transversal pulse is the biggest influence here.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Select the most suitable Signal-MC simulation from a given set of Signal-MC simulations and justify your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of these Gaussian parameterizations are summarized in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ = pd.read_csv(\"./data/for_p4/.csv\")\n",
    "df_\n",
    "sigma, sigma_err, mu, mu_err = tuple(df_[[\"sigma\", \"sigma_err\", \"mu\", \"mu_err\"]].values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize $\\sigma_{\\mathrm{G}}(m_{4\\ell})$ using the existing data.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigma_model(x, *args):\n",
    "    \"\"\"\n",
    "    Model to for sigma parameterization\n",
    "    \n",
    "    \"\"\"\n",
    "    # code\n",
    "    \n",
    "    return x\n",
    "\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood-ratio test\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood ratio test is explicitly used for the selection between the different simulations.\n",
    "For this purpose, the null hypothesis $H_0$, which only corresponds to the background, and the counter hypothesis $H_1$, which corresponds to the background and a signal, are compared with each other.\n",
    "\n",
    "The (extended) (unbinned) likelihood of the $H_0$ hypothesis is based on the parameterization of the background $f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}})$ where $f_{\\mathrm{U}}$ is the previously determined model from the background parameterization with the optimal parameters $\\theta_{\\mathrm{U}}$.\n",
    "\n",
    "The distribution used for the alternative hypothesis can be described by $$f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) = \\mu f_{\\mathrm{S}}(m_{4\\ell}|\\theta_{\\mathrm{S}}) + f_{\\mathrm{U}}(m_{4\\ell}|\\theta_{\\mathrm{U}}) \\, , \\\\ \\int_{\\Omega} f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) \\mathrm{d}m_{4\\ell} = 1 $$ $f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}})$ is a Gaussian distribution $\\mathcal{G}(m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G}}(m_{\\mathrm{H}} ))$ with fixed values for the mass and width. For the estimator $\\mu$ the following holds: $$\\mu \\geq 0 \\, .$$ This restriction leads to a negligible distortion and is introduced for physical considerations, since in the distribution of invariant masses a excess of events is expected if a Higgs boson is present.\n",
    "\n",
    "Thus, the quantity to be considered - the ratio of the likelihoods of both hypotheses - can be formed. For numerical stability, the negative logarithm of this is formed. The factor two is necessary for a later used asymptotic form.\n",
    "\n",
    "$$ q_0 = -2 \\ln \\left( \\frac{\\mathcal{L}(x|\\mu = \\hat{\\mu})}{\\mathcal{L}(x|\\mu = 0)} \\right) \\, .$$ \n",
    "$\\hat{\\mu}$ here is the ideal estimator for a fixed mass and width of the signal hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = hist_range\n",
    "mass_array = np.linspace(lower_bound, upper_bound, 100)\n",
    "sigma_model_arguments = tuple([])\n",
    "sigma_array = sigma_model(mass_array, *sigma_model_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement the likelihood ratio test. For numerical integration, you can use the ['scipy.integrate.quad'](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html) method. For minimizing the likelihood function you can use ['iminuit'](https://nbviewer.jupyter.org/github/scikit-hep/iminuit/blob/master/tutorial/basic_tutorial.ipynb)\n",
    ").\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 9999, (106, 151), [[\"2012\"], [\"A-D\"]]\n",
    "mc_dir = \"../data/mc_aftH/\" \n",
    "ru_dir = \"../data/ru_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "data = raw_data.data\n",
    "data\n",
    "\n",
    "# scale factors for the mc simulations\n",
    "b_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "_, _, background_hist, _ = b_mc_d.variables_for_hist_fit\n",
    "scale_b_mc = np.sum(background_hist)\n",
    "\n",
    "s_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"signal\", mc_dir=mc_dir)\n",
    "_, _, signal_hist, _ = s_mc_d.variables_for_hist_fit\n",
    "scale_s_mc = np.sum(signal_hist)\n",
    "\n",
    "scale_s_mc, scale_b_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iminuit Example:\n",
    "from iminuit import Minuit\n",
    "\n",
    "def extended_likelihood(mu=0.0):\n",
    "    # FUnction that need to be minimized\n",
    "    #code\n",
    "    return mu\n",
    "\n",
    "m = Minuit(extended_likelihood, \n",
    "           mu=0.5, error_mu=0.1, limit_mu=(0.0, 3.0),\n",
    "           errordef=0.5)\n",
    "m.migrad()\n",
    "best_mu = m.values[\"mu\"]\n",
    "\n",
    "\n",
    "q0 = np.zeros(len(mass_array))\n",
    "\n",
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The asymptotic form of the probability density of $q_0$ when no signal is expected is described by\n",
    "$$ f(q_0|\\hat{\\mu}=0) \\rightarrow \\frac{1}{2}\\left( \\delta (q_0) + \\chi^2_{1} \\right) \\, .$$\n",
    "from this the $p_0$ value can be calculated according to:\n",
    "$$ p_0 = \\int\\limits_{q_{0_{\\mathrm{obs}}}}^{\\infty} \\mathrm{d}q_0 f(q_0|\\hat{\\mu}=0) \\, . $$\n",
    "using this, the significance can be estimated as follows: $$Z = \\Phi^{-1}(1-p_0) = \\sqrt{q_{0_{\\mathrm{obs}}}} \\, .$$ However, in this case it should only be considered as a first estimation because the parameterization of the input signal is not accurate.\n",
    "Likewise, the systematics are not considered at this point, only the statistical uncertainty is included in the calculation of $p_0$ or $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the sizes determined in this way can be done in the form of a panda dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q0, p0 = np.zeros(len(sigma_array)), np.zeros(len(sigma_array))\n",
    "df = pd.DataFrame(data=np.array([mass_array, sigma_array, q0, p0, np.sqrt(q0)]).T,\n",
    "                  columns=[\"mass_array\", \"gauss_sigma\", \"q0\", \"p0\", \"sqrt_q0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Visualize the results. For the significance levels the 'get_p0_sigma_lines' from 'PlotHelper' can be used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.stattest.PlotHelper import PlotHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.get_p0_sigma_lines(x_=mass_array, \n",
    "                             ax_obj_=ax, \n",
    "                             max_sigma_=4)\n",
    "ax.plot(df.mass_array, df.p0+1)  # <- change this line accordingly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examination of the final distributions\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the selection of the appropriate signal distribution, different quantities can finally be displayed.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Consider at least the distribution of the four lepton invariant masses, as well as the masses of the two Z-bosons.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Furthermore, the following sizes can also be displayed, although it is questionable whether `z1_index`, `z2_index`, as well as `z1_tag` or `z2_tag` are sizes that require detailed visual observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ProcessHelper.print_possible_variables(\"data/mc_aftH/MC_2012_H_to_ZZ_to_4L_2el2mu_aftH.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.histogramm.HistOf import HistOf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "h = HistOf(mc_dir=\"data/mc_aftH/\", \n",
    "           ru_dir=\"data/ru_aftH/\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "h.variable(\"energy\", 50, (0, 200))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(r\"$p_T$ in GeV\")\n",
    "ax.set_ylabel(\"Bineinträge\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization of the signal MC\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the best suited signal hypothesis, the distribution will be parameterized more precisely. This is done analogously to the background parameterization.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize the signal distribution with a suitable model. Also make sure that the model you choose is the most suitable one.\n",
    "\n",
    "</div>\n",
    "\n",
    "The students can choose from distributions that have already been implemented:\n",
    "- Gauss distribution\n",
    "- Cauchy distribution\n",
    "- Voigt Profile\n",
    "- Single - Sided - Crystal - Ball\n",
    "- Double - Sided - Crystal - Ball\n",
    "\n",
    "\n",
    "However, independently implemented distributions can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.parameterization.SignalFunction import SignalFunction as SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, hist_range, info = 9999, (9999, 9999), [[\"2012\"], [\"A-D\"]]\n",
    "mc_dir = \"./data/mc_aftH/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"signal\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_H_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization \n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determination of statistical significance\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successful parameterization of the background and the signal, the significance of the signal can be determined. The following parameterization is selected for this purpose:\n",
    "$$ f(x, \\alpha_\\mathrm{S}) = \\alpha_\\mathrm{S} f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}}) + (1 - \\alpha_\\mathrm{S}) f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}}) \\, .$$\n",
    "\n",
    "The optimal parameter $\\alpha_{\\mathrm{S}}$ corresponds to the signal probability of this measurement. This determination is carried out with the help of the $-2\\ln\\mathcal{L(\\alpha_{\\mathrm{S}})}$ function which is to be implemented independently. The previously determined parameters $\\theta_{\\mathrm{S}}$ and $\\theta_{\\mathrm{U}}$ of the probability density function of the signal or background are fixed for the determination (profiled Likelihood).\n",
    "\n",
    "The evaluation of the function value $-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)$ should be emphasized here. This corresponds to the assumption that there is no signal component in the existing measurement and the measurement can therefore be explained by the null hypothesis (only background). In asymptotic form, therefore, the significance can be determined analogously to the expression in the chapter on parameterization of the signal MC expression with: $$ Z = \\sqrt{\\frac{-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)}{\\mathcal{L}_{\\mathrm{min}}}} \\, .$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "Implement the $-2\\ln\\mathcal{L(\\alpha_S, m_{4\\ell})}$ function (if not already present).Determine the signal probability plus uncertainty of the measurement.  Specify the significance with which a higgs-like boson is measured. In addition, vary the mass of the higgs-like boson for the two-dimensional variant and compare it with the literature. Visualize and discuss your results.\n",
    "    \n",
    "For the minimization you can use `minuit` as in QRT. The numerical integration can again be done using `scipy.integrate.quad`.\n",
    "    \n",
    "It is also possible to perform an `UninnedFit` using `kafe2` and implement the extended likelihood as a cost function independently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measurement:\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 9999, (106, 151), [[\"2012\"], [\"A-D\"]]\n",
    "mc_dir = \"./data/mc_aftH/\" \n",
    "ru_dir = \"./data/mc_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "raw_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code iminuit\n",
    "\n",
    "def extended_nll_iminuit(alpha_s, mass=125.0):\n",
    "    # code\n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code kafe2\n",
    "\n",
    "def extended_nll_kafe2(data, model, parameter_values, parameter_constraints):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: measurement array\n",
    "    :param model: model array (calculate individual in # code)\n",
    "    :param parameter_values:\n",
    "    :param parameter_constraints: (not used)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if one of the parameter_values is nan\n",
    "    if any(f\"{par}\" == \"nan\" for par in parameter_values):\n",
    "        return np.inf\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    _total_log_likelihood = 0.0 # your code\n",
    "    \n",
    "    # check if _total_log_likelihood is nan\n",
    "    if np.isnan(_total_log_likelihood):\n",
    "        return np.inf\n",
    "    return -2.0 * _total_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D profiled Likelihood\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-dimensional variant only varies the signal probability. The mass is held after the parameterization and is not varied anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (iminuit minimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.stattest.PlotHelper import PlotHelper\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_ = np.linspace(0.0, 0.8, 1000)\n",
    "nll_ = extended_nll(alpha_s=alpha_, mass=125)\n",
    "\n",
    "# as example (to be replaced with actual nll array)\n",
    "nll_ = 150*(alpha_ - 0.4) ** 2\n",
    "\n",
    "z_ = np.sqrt(nll_[0])\n",
    "\n",
    "# code to further customize the plot\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.plot_nll_scan_main_window(ax_obj_=ax, \n",
    "                                     x_=alpha_, \n",
    "                                     y_=nll_, \n",
    "                                     z_=z_)\n",
    "\n",
    "ax_in = inset_axes(ax, width='50%', height='40%', loc=\"upper right\")\n",
    "PlotHelper.plot_nll_scan_inside_window(ax_obj_=ax_in, \n",
    "                                       x_=alpha_, y_=nll_, \n",
    "                                       x_ticks_=PlotHelper.get_error_points_from_nll(x_=alpha_, nll_=nll_))\n",
    "mark_inset(ax, ax_in, loc1=2, loc2=4, fc=\"none\", ec=\"grey\", lw=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 'kafe2' variant, an 'uninned container' must be created and adjusted. For the 1D variant it is recommended to use the method `fix_parameter(par_name, value)`.\n",
    "The significance can be calculated using the implemented `extended_nll_kafe2` cost function. It is recommended that you use the `limit_parameter` to restrict the parameter `alpha` to a reasonable interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kafe2 as K2\n",
    "\n",
    "my_cost_function = K2.UnbinnedCostFunction_UserDefined(extended_nll_kafe2)\n",
    "\n",
    "# more code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D profiled Likelihood - Mass determination\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the mass is also determined. This is done by the two-dimensional variation - mass and signal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the 1D version, the best mass can be determined at the same time. The procedure is analogous to the 1D variant/QRT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code:\n",
    "#     - minimization\n",
    "#     - plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference to the 1D variant is that the mass is no longer fixed by `fix_parameter`, but is varied in a certain interval (`limit_parameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-21T13:24:49.031481Z",
     "start_time": "2020-05-21T13:24:49.028986Z"
    }
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of the measurements: CMS and ATLAS\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a better statistical significance, it is advantageous to include further measurements in the overall assessment. This can be done either by adding more data or by another experiment. Within the scope of this task the second option is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ATLAS experiment, therefore, similar steps as above would have to be carried out, although many of the cuts show deviations. To shorten the work a bit, the final measurement is provided, as well as the histogram entries of the MC simulations.\n",
    "\n",
    "In addition, the parameters of the most suitable functions (linear combination of centered legend trepolynomials up to the third order for the background and the DSCB distribution for the signal) are provided (but it is also possible to check these results yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kafe2 as K2\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_data_dir_ = \"../data/for_p4/\"\n",
    "atlas_files = [os.path.join(atlas_data_dir_, file) for file in os.listdir(atlas_data_dir_) if \".csv\" in file and \"atlas\" in file]\n",
    "pprint(atlas_files)\n",
    "hist_range_ = (106, 151)\n",
    "num_bins = 45\n",
    "\n",
    "data_A = np.loadtxt(atlas_files[2])\n",
    "data_A = data[(data >= hist_range_[0]) & (data <= hist_range_[1])]\n",
    "\n",
    "mc_bac = np.loadtxt(atlas_files[1], delimiter=\",\").T\n",
    "mc_bac_data = mc_bac[0]\n",
    "mc_bac_error = mc_bac[1]\n",
    "\n",
    "mc_sig = np.loadtxt(atlas_files[0], delimiter=\",\").T\n",
    "mc_sig_data = mc_sig[0]\n",
    "mc_sig_error = mc_sig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_CMS = data\n",
    "data_ATLAS = data_A\n",
    "\n",
    "background_parameter_CMS = background_parameter\n",
    "signal_parameter_CMS = signal_parameter\n",
    "\n",
    "background_parameter_ATLAS  = np.array([0.025050083639971264, \n",
    "                                        0.00018744120591615378, \n",
    "                                        -1.178181555551107e-05, \n",
    "                                        1.1872474265844408e-07])\n",
    "signal_parameter_ATLAS = np.array([2.1395802886491615, \n",
    "                                   124.19274632605674, \n",
    "                                   0.8324353801486924, \n",
    "                                   1.8034218165656613, \n",
    "                                   14.109501070824912, \n",
    "                                   19.9999999999894])\n",
    "bfu_ATLAS = llc_background.grade_3\n",
    "sfu_ATLAS = sf.DSCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Combine the two measurements and determine the mass of the Higgs boson resulting from the combination, the signal probability, as well as the significance that a Higgs boson was measured in the recordings.\n",
    "\n",
    "You can reuse/modify the functions you have already implemented.\n",
    "    \n",
    "Note:\n",
    "    $$ \\mathcal{L_{\\mathrm{tot}}(\\alpha_S, m_{4\\ell})} = \\mathcal{L_{1}(\\alpha_S, m_{4\\ell})} \\mathcal{L_{2}(\\alpha_S, m_{4\\ell})} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code (1D and 2D minimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
