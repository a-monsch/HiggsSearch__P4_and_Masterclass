{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "version 0.0.2.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from include.RandomHelper import check_data_state\n",
    "check_data_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {return false;}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Events\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section the CMS detector will be discussed in more detail. This is an important subject, because the used data originates from this detector and introduced cuts on the data sets can be explained, for example, by the geometry of the detector. The visualization is done with the CMS own web interface: the [Ispy-WebGL](https://github.com/cms-outreach/ispy-webgl)\n",
    "\n",
    "The CMS detector is a multi-purpose detector and consists of several components which are used for different particle detection. Select from the collection relevant events that could contain decays of the Higgs boson and which show the functionality of the individual detector components.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "Possible decays contained in the collection are: \n",
    "\n",
    "- $\\mathrm{H} \\rightarrow \\mathrm{ZZ} \\rightarrow 4\\ell$\n",
    "- $\\mathrm{H} \\rightarrow \\gamma \\gamma$\n",
    "- $\\mathrm{H} \\rightarrow \\mathrm{W}^+ \\mathrm{W}^- \\rightarrow 4\\ell$\n",
    "\n",
    "Select two examples each and save them as an image. How would the typical signature of these individual decays appear in the detector?\n",
    "</div>\n",
    "\n",
    "The switch-on functions of individual detector components and the option to save the individual events as images can be performed within the web interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an Internet connection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<iframe src=\"https://ispy-webgl.web.cern.ch/ispy-webgl/\" width=\"100%\" height=\"700\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without an internet connection: Open the `index.html` from the [Github Repository](https://github.com/cms-outreach/ispy-webgl) locally in a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, only events from the decay into four leptons (muons and electrons, not tau leptons) are considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Data format\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The data format used in the following, from which all necessary sizes are taken, is a modified .CSV format. The advantage of this format is that the data can be viewed at any time in the same way as a human would read it.\n",
    "\n",
    "This section serves to familiarize with the data format in order to be able to perform the task in the following section.\n",
    "\n",
    "The separation of individual variables in an event is done using \";\". The entries of the individual leptons within an event, on the other hand, are classically separated with \",\". This gives the advantage of individual numbers of leptons in an event without the introduction of additional placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#%matplotlib qt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "name_1 = \"../data/for_long_analysis/mc_init/MC_2012_ZZ_to_4L_2el2mu_init.csv\"\n",
    "name_2 = \"../data/for_long_analysis/mc_init/MC_2012_ZZ_to_4L_4mu_init.csv\"\n",
    "\n",
    "dataframe_1 = pd.read_csv(name_1, delimiter=\";\")\n",
    "dataframe_2 = pd.read_csv(name_2, delimiter=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataframe_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dataframe_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For processing individual sizes, the respective elements from the string (`str`) data format can be converted back into a list (`list`):\n",
    "For this purpose the options are available either the standard library `ast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "px = ast.literal_eval(f\"[{dataframe_2.loc[3, 'px']}]\")\n",
    "px = np.array(px, dtype=float)\n",
    "px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "or to use the `split` method for the `str`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "py = dataframe_2.loc[3, 'py'].split(\",\")\n",
    "py = np.array(py, dtype=float)\n",
    "py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "From this, new quantities such as the transverse momentum can be determined, on the basis of which a limitation in the quantities can and should be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pt = np.sqrt(px ** 2 + py ** 2)\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For muons, a minimum value of 5 GeV for the transverse impulse must be exceeded. For transverse impulse values below this, the probability of muon misidentification rises.\n",
    "In this respect, all muons in this event that do not meet the condition must be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pt_minimum_filter = pt > 5\n",
    "pt_minimum_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pt = pt[pt_minimum_filter]\n",
    "pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This application of the resulting filter must therefore be applied individually for an event to each variable in the data set and the processed event must be saved again, which is far too time-consuming for a single, manual application, but can be significantly reduced by introducing an already created class that does the whole thing automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of the filters by using custom Apply Class\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A filter is applied to all variables within an event and is different from the filter for the next event.\n",
    "The application of the filter to the data set in series is already implemented and can be applied via the Apply class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.Apply import Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the Apply object is created, an \"allowed\" and a \"calculation\" class instance must also be passed. The required variables or filters for the application of the filters are calculated using these classes.\n",
    "For the example of the minimum transverse impulse the two classes can be represented as a collection of functions, which could stand alone (`@staticmethod`), but logically combined in one (two) class(es)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Calc_Start(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def pt(px, py):\n",
    "        return np.sqrt(px ** 2 + py ** 2)\n",
    "\n",
    "class Allowed_Start(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def min_pt(pt, look_for):\n",
    "        return pt > 7.0 if look_for == \"electron\" else (\n",
    "            pt > 5.0 if look_for == \"muon\" else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(name_2)\n",
    "process = Apply(input_=name_2, particle_type=\"muon\", use_n_rows=10000,\n",
    "                calc_instance=Calc_Start, allowed_instance=Allowed_Start,\n",
    "                use_swifter=True, multi_cpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data set `name_2` the variable of the transverse pulse does not yet exist. In an intermediate step, which is already contained in the filter `\"check_min_pt\"`, the transverse pulse can also be added explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.run(\"add_pt\")\n",
    "process.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram of such a variable can be displayed by the method `hist_of_variable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.hist(variable=\"pt\", bins=100, hist_range=(0, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the filter for the minimum transverse impulse and removing all subsequent events containing less than four leptons provides the appropriate distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process.run(\"filter_pt_minimum\")\n",
    "process.hist(variable=\"pt\", bins=100, hist_range=(0, 80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other variables can also be visualized by the histogram.\n",
    "The performed cut mainly influences the distribution of the transverse impulse, but changes little in the distribution of other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Examine the distributions of some of the quantities present in the records.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here goes the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Calculation of important variables\n",
    "--------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Following the example of the transversal impulse, implement all subsequently necessary quantities and visualize them appropriately:\n",
    "  * Pseudorapidity $\\eta$\n",
    "  * Transverse pulse $p_T$\n",
    "  * Azimuthal angle $\\phi$\n",
    "\n",
    "And explain your observations.\n",
    "\n",
    "(Check their implementations using MC simulations of the background)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The class skeleton used for this inherits the previous method for the transverse momentum and an initial class containing methods such as the reconstruction of Z-boson pairs are added afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CalcStudent(Calc_Start):\n",
    "    '''\n",
    "    Class for the calculation of certain sizes that are used for\n",
    "    the cuts or are essential for the reconstruction.\n",
    "    '''\n",
    "\n",
    "    @staticmethod\n",
    "    def combined_charge(charge, combine_num):\n",
    "        '''\n",
    "        Tests whether an electrically neutral charge combination is possible.\n",
    "\n",
    "        :param charge: ndarray\n",
    "                       1D array containing data with \"int\" type.\n",
    "        :param combine_num: int\n",
    "                            4 if look_for is not \"both\", 2 else\n",
    "        :return: bool\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def eta(px=None, py=None, pz=None, energy=None):\n",
    "        '''\n",
    "        Calculates the pseudorapidity.\n",
    "        Optional with or without energy.\n",
    "\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param pz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param energy: ndarray\n",
    "                       1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def invariant_mass_square(px, py, pz, energy=None, eta=None, phi=None):\n",
    "        '''\n",
    "        Calculates the square of the invariant mass.\n",
    "        Optional with or without energy.\n",
    "        Optionally with or without eta and phi.\n",
    "\n",
    "        :param phi: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param pz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param energy: ndarray\n",
    "                       1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def phi(px, py):\n",
    "        '''\n",
    "        Calculation of the angle phi.\n",
    "\n",
    "        :param px: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :param py:  ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_phi(phi1, phi2):\n",
    "        '''\n",
    "        Calculation of the difference between two phi angles.\n",
    "\n",
    "        :param phi1: ndarray\n",
    "                     1D array containing data with \"float\" type.\n",
    "        :param phi2: ndarray\n",
    "                     1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def delta_r(eta, phi):\n",
    "        '''\n",
    "        Calculation of delta_r.\n",
    "\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param phi: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"float\" type.\n",
    "        '''\n",
    "        # code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the cuts\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogous to the cut to the minimum transverse pulse, more cuts can be introduced which are then applied later. An example of which cuts were used by CMS can be found in the [official publication] (https://arxiv.org/pdf/1207.7235.pdf).\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement the methods listed in the class `AllowedStudent`. Estimate the choice of cuts using the distributions in the chapter **Application of the filters by using custom Apply Class**, the **detector** and the **kinematic constraints** of the events.\n",
    "\n",
    "(Check your implementations using the MC simulations of the background).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class AllowedStudent(Allowed_Start):\n",
    "    '''\n",
    "    Class that introduces certain cuts and thus restricts the leptons in the events.\n",
    "    '''\n",
    "    \n",
    "    @staticmethod\n",
    "    def lepton_type(classification, particle_type):\n",
    "        '''\n",
    "        Checks if electrons or muons satisfy the classification condition.\n",
    "        \n",
    "        :param classification: ndarray\n",
    "                               1D array containing data with `str` type.\n",
    "        :param particle_type: str\n",
    "                              \"muon\" or \"electron\"\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        #code\n",
    "    \n",
    "    @staticmethod\n",
    "    def delta_r(delta_r):\n",
    "        '''\n",
    "        Checks if delta_r is smaller than the allowed value.\n",
    "\n",
    "        :param delta_r: ndarray\n",
    "                        1D array containing data with `float` type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def rel_pf_iso(rel_pf_iso):\n",
    "        '''\n",
    "        Checks if rel_pf_iso is smaller than the allowed value.\n",
    "\n",
    "        :param rel_pf_iso: ndarray\n",
    "                           1D array containing data with `float` type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def misshits(misshits):\n",
    "        '''\n",
    "        Checks if the minimum number of misshits was kept.\n",
    "\n",
    "        :param misshits:\n",
    "        :return:\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def pt(p_t, look_for, coll_size=4):\n",
    "        '''\n",
    "        Checks if the exact pedingun regarding pt is observed.\n",
    "        (>20 GeV: >= 1; >10 GeV: >= 2; >Minimum pt: >= 4).\n",
    "\n",
    "        :param p_t: ndarray\n",
    "                    1D array containing data with `float` type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :param coll_size: int\n",
    "                          4 if look_for is not \"both\", 2 else\n",
    "        :return: ndarray\n",
    "                 1D array containing data with `bool` type.\n",
    "        '''\n",
    "        # code\n",
    "        \n",
    "    @staticmethod\n",
    "    def eta(eta, look_for):\n",
    "        '''\n",
    "        Checks if the pseudorapidity of leptons is valid.\n",
    "\n",
    "        :param eta: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def lepton_type(typ, look_for):\n",
    "        '''\n",
    "        Checks for the permitted classification of leptons.\n",
    "\n",
    "        :param typ: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param look_for: str\n",
    "                         \"muon\"; \"electron\" or \"both\"\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def impact_param(sip3d, dxy, dz):\n",
    "        '''\n",
    "        Checks if the impact parameters of the collision are valid and sorts out\n",
    "        events that do not have a clear and equal collision point.\n",
    "\n",
    "        :param sip3d: ndarray\n",
    "                      1D array containing data with \"float\" type.\n",
    "        :param dxy: ndarray\n",
    "                    1D array containing data with \"float\" type.\n",
    "        :param dz: ndarray\n",
    "                   1D array containing data with \"float\" type.\n",
    "        :return: ndarray\n",
    "                 1D array containing data with \"bool\" type.\n",
    "        '''\n",
    "        # code\n",
    "\n",
    "    @staticmethod\n",
    "    def zz(z1, z2):\n",
    "        '''\n",
    "        Checks if the Z1 candidate and the Z2 candidate is within the allowed range.\n",
    "\n",
    "        :param z1: float\n",
    "        :param z2: float\n",
    "        :return: bool\n",
    "        '''\n",
    "        # code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combination of the implemented code with the partially provided classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.CalcAndAllowerInit import AllowedInit\n",
    "from include.processing.CalcAndAllowerInit import CalcInit\n",
    "\n",
    "AllowedInit.a_allowed_instance = AllowedStudent\n",
    "AllowedInit.a_calc_instancea = CalcStudent\n",
    "class Allowed(AllowedStudent, AllowedInit):\n",
    "    pass\n",
    "    \n",
    "CalcInit.c_allowed_instance = Allowed\n",
    "CalcInit.c_calc_instance = CalcStudent\n",
    "class Calc(CalcStudent, CalcInit):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application of filters and reconstruction on MC - simulations\n",
    "---------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing implementations of the cuts should be tested using the background MC simulations. This is to prevent the targeted work towards a certain goal (actual measurement), since a small number of later events in the actual measurement should avoid the fact of subjectively selecting specific events in an area as far as possible.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Now apply all the cuts you have implemented to the MC simulations and measurements.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Therefore it is useful to create a list of tuples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.processing.ApplyHelper import ProcessHelper\n",
    "\n",
    "\n",
    "# All Background MC and signal MC for m_H = 125 GeV\n",
    "mc_files = not True\n",
    "# actual measurement\n",
    "run_files = True\n",
    "\n",
    "# folder with initial records: measurement\n",
    "dir_measurement =\"../data/for_long_analysis/ru_init/\"\n",
    "# folder with initial records: backround MC and singal (125 GeV) MC\n",
    "dir_mc = \"../data/for_long_analysis/mc_init/\"\n",
    "\n",
    "file_tuples = []\n",
    "if mc_files:\n",
    "    file_tuples += ProcessHelper.create_tuple(dir_mc)    \n",
    "\n",
    "if run_files:\n",
    "    file_tuples += ProcessHelper.create_tuple(dir_measurement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single such `namedtuple` contains the file and particle type of the data set (necessary for `Apply`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_tuples[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole filter and reconstruction process can be combined in one function. The order of the filters is decisive for the runtime, but cannot be changed freely, since there is a logical process behind the application of the filters and reconstructions that cannot be changed freely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the previously implemented new variables and the cuts, the following filter and reconstruction steps are available for selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Apply.help()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function can have the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# possible operations\n",
    "Apply.help()\n",
    "\n",
    "def filter_and_reco_process(used_pair):\n",
    "    process = Apply(input_=used_pair.name, \n",
    "                    particle_type=used_pair.particle, \n",
    "                    multi_cpu=False,\n",
    "                    calc_instance=Calc, \n",
    "                    allowed_instance=Allowed)\n",
    "    \n",
    "    # Logical order selection\n",
    "    # quicksave: Saves the data set AFTER the application \n",
    "    # of the filter or reconstruction step\n",
    "    \n",
    "    \n",
    "    \n",
    "    process.run(\"filter_type\", quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftT\"))\n",
    "    \n",
    "    # ProcessHelper.change_on_affix(\"Name_OldAffix.csv\", \"NewAffix\"):\n",
    "    # -> This method changes \"Name_OldAffix.csv\" in \"Name_NewAffix.csv\"\n",
    "    #    and saves \"Name_NewAffix.csv\" in the new directory <ru or mc>_NewAffix\n",
    "    \n",
    "    process.run(\"filter_pt_minimum\")\n",
    "    if process.particle_type != \"muon\":\n",
    "        process.run(\"filter_misshit\")\n",
    "    process.run(\"filter_electric_charge\")\n",
    "    process.run(\"filter_impact_param\")\n",
    "    process.run(\"reconstruct_4_lepton_mass_from_zz\", quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftH\"))\n",
    "    process.run(\"filter_4_lepton_mass\")\n",
    "    process.run(\"filter_pt_exact\")\n",
    "    process.run(\"filter_eta\")\n",
    "    process.run(\"filter_electric_charge\")\n",
    "    process.run(\"filter_4_lepton_mass\", quicksave=ProcessHelper.change_on_affix(used_pair.name, \"befZ\"))\n",
    "    process.run(\"reconstruct_zz\", quicksave=ProcessHelper.change_on_affix(used_pair.name, \"aftZ\"))\n",
    "    process.run(\"filter_2_lepton_mass\")\n",
    "    process.run(\"filter_rel_iso\")\n",
    "    del process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Select the appropriate sequence of filter and reconstruction steps.\n",
    "\n",
    "Do some filter steps have to be performed several times?and why is it not useful to perform the reconstruction of the ZZ bosons very early?\n",
    "\n",
    "(If you are interested in a time-optimized version with the functions available here, the speed of the individual filters can be evaluated in a procedure: Use the data set of the underground MC simulations. When initializing the `Apply` instance use the following `kwargs`: `multi_cpu=False`. Now you should see the speed displayed next to it, which shows how many lines (events) per second are being processed.)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The application of `filter_and_reco_process` to the records: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "if input(\"Run all filter + reco (y/n): \") == \"y\":\n",
    "    for pair in tqdm(file_tuples):\n",
    "        filter_and_reco_process(pair)\n",
    "        clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Parameterization of the background\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To avoid fluctuations, it is advantageous to express the background distribution of the MC simulations by a probability density function.\n",
    "\n",
    "The difference between a fit and a parameterization should again be clarified: The application of an fit always occurs when a certain theory previously established for it is to be confirmed or falsified by the measured data. The data contained for the fitting have uncertainty, which depends on the experiment and the measurement methods, but is fixed mainly by the measurement of the data.\n",
    "\n",
    "In the case of a parameterization, the aim is to express the existing measurement, which in this example is available in the form of a histogram, by a different form. In this case it should be a continuous function - more precisely: a continuous probability density function (pdf) in a certain interval.\n",
    "\n",
    "Thus, the main difference between an adaptation and a parameterization is clear: While in the first case a dependence given by the theory is present, in the second case an attempt is made to find a parameterization that describes the data in another form as accurately as possible.\n",
    "\n",
    "In this sense - and especially in this case - it is not possible to find an exact function for this. In order to be able to make a choice nevertheless, the following procedure can be followed: For a $\\chi^2$ - fitting the uncertainties on the data are decisive, whether a suitable model can be chosen for the data ($\\frac{\\chi^2}{n_{\\mathrm{df}}}$) or not. Conversely, the uncertainties on the data can be scaled until a model matches. The question now, which model parameterizes the data better can be answered with the scaling factor.\n",
    "\n",
    "However, not only the smallest scaling factor is the decisive point, since this can always be realized by a polynomial of a very high degree. The so-called overfitting should be avoided, because only the data behaviour should be parameterized and not the fluctuation observed in this measurement. The choice should therefore be made for a function which still contains a sufficiently small number of parameters, but already keeps the scaling factor for the uncertainty as small as possible.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Use a suitable model to parameterize the background. Also make sure that the model you choose is the most suitable one.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The implementation of linear combinations of the legendre polynomials has already been carried out and summarized in the class `LLC`. Independent functions can also be used (see the numpy documentation).\n",
    "\n",
    "The use of linear combinations of legendre polynomials offers the advantage over normal polynomials that the legendre polynomials form an orthogonal function system and behave robustly in case of an fit - unlike normal polynomials. The correlation of the uncertainties on the individual parameters is reduced due to the orthogonality, whereas with normal polynomials there is almost always a strong correlation between the individual parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.parameterization.LegendrePoly import LegendreLinearCombination as LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "llc_background = LLC(x_mean=0.0) # centering around x_mean\n",
    "\n",
    "x_ = np.linspace(-1, 1, 1000)\n",
    "plt.plot(x_, llc_background.grade_0(x_, a=1))\n",
    "plt.plot(x_, llc_background.grade_1(x_, a=1, b=1))\n",
    "plt.plot(x_, llc_background.grade_2(x_, a=1, b=1, c=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In preparation, the number of bin numbers and the considered interval should be selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\"  \n",
    "llc_background = LLC(x_mean=(999.1 + 999.2)/2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>\n",
    "\n",
    "The uncertainties thus calculated are fundamentally different from the $\\sqrt{N}$ variant - they describe the scaled fluctuation of the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"background\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.fits.McFitInit import McFitInit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir, info=info)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# parametrization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Selection of the appropriate signal MC\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The goal of this section is to narrow down the possible signal MC simulations and to decide on a single one and continue working with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Preliminary parameterization of signal MC\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Different signal MC simulations are available. These were roughly parameterized by a Gaussian distribution $\\mathcal{G} \\left( m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G}} \\right)$. The aim is to determine the dependence of the width on the invariant mass and to describe it by a suitable model. The resolution of the detector's transversal pulse is the biggest influence here.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Select the most suitable Signal-MC simulation from a given set of Signal-MC simulations and justify your choice.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The results of these Gaussian parameterizations are summarized in a Pandas Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_ = pd.read_csv(\"../data/for_long_analysis/other_mc_gauss_param/gauss_params.csv\")\n",
    "df_\n",
    "sigma, sigma_err, mu, mu_err = tuple(df_[[\"sigma\", \"sigma_err\", \"mu\", \"mu_err\"]].values.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize $\\sigma_{\\mathrm{G}}(m_{4\\ell})$ using the existing data.\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def sigma_model(x, *args):\n",
    "    \"\"\"\n",
    "    Model to for sigma parameterization\n",
    "    \n",
    "    \"\"\"\n",
    "    # code\n",
    "    \n",
    "    return x\n",
    "\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This variant - the continuous parameterization of the signal MC does not happen in the correct analysis and is used here mainly for time reasons. Normally, a separate MC simulation is created for each Higgs mass, with which all further steps are performed. However, creating such a simulation is extremely time-consuming. A linear interpolation of a Gaussian-like parameterization, however, already leads to a sufficient result, that we want to achieve in the context of this section: The justification of the choice of a certain signal MC simulation based on the measurement - as well as an estimation of the statistical significance (more about this in the next sections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Likelihood-ratio test\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The likelihood ratio test is explicitly used for the selection between the different simulations.\n",
    "For this purpose, the null hypothesis $H_0$, which only corresponds to the background, and the counter hypothesis $H_1$, which corresponds to the background and a signal, are compared with each other.\n",
    "\n",
    "The extended unbinned likelihood of the $H_0$ hypothesis is based on the parameterization of the background $f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}})$ where $f_{\\mathrm{U}}$ is the previously determined model from the background parameterization with the optimal parameters $\\theta_{\\mathrm{U}}$.\n",
    "\n",
    "The distribution used for the alternative hypothesis can be described by $$f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) = \\mu f_{\\mathrm{S}}(m_{4\\ell}|\\theta_{\\mathrm{S}}) + f_{\\mathrm{U}}(m_{4\\ell}|\\theta_{\\mathrm{U}}) \\, , \\\\ \\int_{\\Omega} f_{\\mu\\mathrm{S+B}}(m_{4\\ell}|\\mu) \\mathrm{d}m_{4\\ell} = 1 $$ $f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}})$ is a Gaussian distribution $\\mathcal{G}(m_{4\\ell}, m_{\\mathrm{H}}, \\sigma_{\\mathrm{G}}(m_{\\mathrm{H}} ))$ with fixed values for the mass and width. For the estimator $\\mu$ the following holds: $$\\mu \\geq 0 \\, .$$ This restriction leads to a negligible distortion and is introduced for physical considerations, since in the distribution of invariant masses a excess of events is expected if a Higgs boson is present.\n",
    "\n",
    "Thus, the quantity to be considered - the ratio of the likelihoods of both hypotheses - can be formed. For numerical stability, the negative logarithm of this is formed. The factor two is necessary for a later used asymptotic form.\n",
    "\n",
    "$$ q_0 = -2 \\ln \\left( \\frac{\\mathcal{L}(x|\\mu = \\hat{\\mu})}{\\mathcal{L}(x|\\mu = 0)} \\right) \\, .$$ \n",
    "$\\hat{\\mu}$ here is the ideal estimator for a fixed mass and width of the signal hypothesis.\n",
    "\n",
    "The test statistic $q_0$ used here is a simplified version of the test statistic used at the LHC for this analysis. It corresponds to the test statistics used at the LEP and has the disadvantage of the bias, that is already mentioned. However, the advantage of the test statistic is that it is easier to implement and the calculation takes less time because no Toy Monte Carlo simulations are necessary. Likewise, even for a small number of events the analytical form can be used, which the test statistic takes in the borderline case of many events.\n",
    "\n",
    "Remember: The extended likelihood function is defined as follows $$ \\mathcal{L}(x|\\mu) = \\frac{\\nu(\\mu)^N}{N!}\\mathrm{e}^{-\\nu(\\mu)}\\prod_i^N f_{\\mu\\mathrm{S+B}}(x_i|\\mu) $$ and differs in the scaling factor from the normal likelihood function. Due to this scaling factor the number of measurements (since it is a counting experiment) plays a certain role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lower_bound, upper_bound = hist_range\n",
    "mass_array = np.linspace(lower_bound, upper_bound, 100)\n",
    "sigma_model_arguments = tuple([])\n",
    "sigma_array = sigma_model(mass_array, *sigma_model_arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Implement the likelihood ratio test. For numerical integration, you can use the ['scipy.integrate.quad'](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.quad.html) method. For minimizing the likelihood function you can use ['iminuit'](https://nbviewer.jupyter.org/github/scikit-hep/iminuit/blob/master/tutorial/basic_tutorial.ipynb)\n",
    ").\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# measurement\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 9999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\" \n",
    "ru_dir = \"../data/for_long_analysis/ru_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "data = raw_data.data\n",
    "data\n",
    "\n",
    "# scale factors for the mc simulations\n",
    "b_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "_, _, background_hist, _ = b_mc_d.variables_for_hist_fit\n",
    "scale_b_mc = np.sum(background_hist)\n",
    "\n",
    "s_mc_d = McFitInit(bins=bins, hist_range=hist_range, tag=\"signal\", mc_dir=mc_dir)\n",
    "_, _, signal_hist, _ = s_mc_d.variables_for_hist_fit\n",
    "scale_s_mc = np.sum(signal_hist)\n",
    "\n",
    "scale_s_mc, scale_b_mc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Iminuit Example:\n",
    "from iminuit import Minuit\n",
    "\n",
    "def extended_likelihood(mu=0.0):\n",
    "    # FUnction that need to be minimized\n",
    "    #code\n",
    "    return mu\n",
    "\n",
    "m = Minuit(extended_likelihood, \n",
    "           mu=0.5, error_mu=0.1, limit_mu=(0.0, 3.0),\n",
    "           errordef=0.5)\n",
    "m.migrad()\n",
    "best_mu = m.values[\"mu\"]\n",
    "\n",
    "\n",
    "q0 = np.zeros(len(mass_array))\n",
    "\n",
    "#code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The asymptotic form of the probability density of $q_0$ when no signal is expected is described by\n",
    "$$ f(q_0|\\hat{\\mu}=0) \\rightarrow \\frac{1}{2}\\left( \\delta (q_0) + \\chi^2_{1} \\right) \\, .$$\n",
    "from this the $p_0$ value can be calculated according to:\n",
    "$$ p_0 = \\int\\limits_{q_{0_{\\mathrm{obs}}}}^{\\infty} \\mathrm{d}q_0 f(q_0|\\hat{\\mu}=0) \\, . $$\n",
    "using this, the significance can be estimated as follows: $$Z = \\Phi^{-1}(1-p_0) = \\sqrt{q_{0_{\\mathrm{obs}}}} \\, .$$ However, in this case it should only be considered as a first estimation because the parameterization of the input signal is not accurate.\n",
    "Likewise, the systematics are not considered at this point, only the statistical uncertainty is included in the calculation of $p_0$ or $Z$.\n",
    "\n",
    "In 2012, the particle excess and the assignment of this excess to a Higgs boson which then led to its discovery was carried out using this method. In the following, another method for determining significance is presented, which leads to similar significance - altough with different assumptions - as in this part of the task and provides a good crosscheck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Saving can be done in the form of a panda dataframe, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "q0, p0 = np.zeros(len(sigma_array)), np.zeros(len(sigma_array))\n",
    "df = pd.DataFrame(data=np.array([mass_array, sigma_array, q0, p0, np.sqrt(q0)]).T,\n",
    "                  columns=[\"mass_array\", \"gauss_sigma\", \"q0\", \"p0\", \"sqrt_q0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Visualize the results. For the significance levels the 'get_p0_sigma_lines' from 'PlotHelper' can be used.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from include.stattest.PlotHelper import PlotHelper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.get_p0_sigma_lines(x_=mass_array, \n",
    "                             ax_obj_=ax, \n",
    "                             max_sigma_=4)\n",
    "ax.plot(df.mass_array, df.p0+1)  # <- change this line accordingly\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Examination of the final distributions\n",
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After the selection of the appropriate signal distribution, different quantities can finally be displayed.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Consider at least the distribution of the four lepton invariant masses, as well as the masses of the two Z-bosons.\n",
    "    \n",
    "</div>\n",
    "\n",
    "Furthermore, the following sizes can also be displayed, although it is questionable whether `z1_index`, `z2_index`, as well as `z1_tag` or `z2_tag` are sizes that require detailed visual observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.processing.ApplyHelper import ProcessHelper\n",
    "print(ProcessHelper.print_possible_variables(\"../data/for_long_analysis/mc_aftH/MC_2012_H_to_ZZ_to_4L_2el2mu_aftH.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.histogramm.HistOf import HistOf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "h = HistOf(mc_dir=\"../data/for_long_analysis/mc_aftH\",\n",
    "           ru_dir=\"../data/for_long_analysis/ru_aftH\", info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# example\n",
    "h.variable(\"energy\", 50, (0, 200))\n",
    "ax = plt.gca()\n",
    "ax.set_xlabel(r\"$p_T$ in GeV\")\n",
    "ax.set_ylabel(\"Entries\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The uncertainties presented here are asymmetrical. For this purpose, a Poisson distribution (measured number of events) is determined from the expected value, the lower limit ( - 34%) and the upper limit (+34%), so that a 68% uncertainty interval can be specified. The asymmetry of the Poisson distribution is clearly visible for a small expected value. Only in the limit case of large expected value does the Poisson distribution transition to the Gaussian distribution and the uncertainties on a measured value become symmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# other histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameterization of the signal MC\n",
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After choosing the best suited signal hypothesis, the distribution will be parameterized more precisely. This is done analogously to the background parameterization.\n",
    "\n",
    "The motivation is analogous to that given in the section on background parameterization. Only the function forms used for parameterization change: These are bell-shaped in this case.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Parameterize the signal distribution with a suitable model. Also make sure that the model you choose is the most suitable one.\n",
    "\n",
    "</div>\n",
    "\n",
    "The students can choose from distributions that have already been implemented:\n",
    "- Gauss distribution\n",
    "- Cauchy distribution\n",
    "- Voigt Profile\n",
    "- Single - Sided - Crystal - Ball\n",
    "- Double - Sided - Crystal - Ball\n",
    "\n",
    "\n",
    "However, independently implemented distributions can also be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from include.parameterization.SignalFunction import SignalFunction as SF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variant:\n",
    "\n",
    " 1. <ins>The data from the histogram is raw output.</ins> \n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "    * Calculate the uncertainties on the individual entries of the unscaled histogram\n",
    "    * Scale the histograms of the individual channels and combine them (and the calculated combined uncertainties) in a common histogram.\n",
    "\n",
    "    (The individual parts can be accessed via the 'data' attribute, where it is a nested 'dict'. The first level contains the three individual channels. Each of the channels then contains the unscaled histogram entries, the correction factor and the data used for histogram creation).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mc_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir).get_mc_raw(tag=\"signal\")\n",
    "print(list(raw_mc_data.data.keys()))\n",
    "raw_mc_data.data[\"mc_track_H_ZZ_4el_2012\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization \n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. <ins>The data from the histogram is a summarized output</ins> \n",
    "    <div class=\"alert alert-info\">\n",
    "     \n",
    "    Use the already combined values for a histogram fit.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_data = McFitInit(bins=bins, hist_range=hist_range, tag=\"background\", mc_dir=mc_dir)\n",
    "bins, hist_range, hist_entries, combined_hist_entries_errors = format_data.variables_for_hist_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual fit is done by using the histogram fit implemented in 'kafe2'. An example of a histogram fit can be found [here](https://kafe2.readthedocs.io/en/latest/parts/user_guide.html#example-5-histogram-fit)\n",
    "\n",
    "It is advantageous to create a function or class to avoid lengthy repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametrization\n",
    "import kafe2 as K2\n",
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Determination of statistical significance\n",
    "-------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After successful parameterization of the background and the signal, the significance of the signal can be determined. The following parameterization is selected for this purpose:\n",
    "$$ f(x, \\alpha_\\mathrm{S}) = \\alpha_\\mathrm{S} f_{\\mathrm{S}}(x|\\theta_{\\mathrm{S}}) + (1 - \\alpha_\\mathrm{S}) f_{\\mathrm{U}}(x|\\theta_{\\mathrm{U}}) \\, .$$\n",
    "\n",
    "The optimal parameter $\\alpha_{\\mathrm{S}}$ corresponds to the signal probability of this measurement. This determination is carried out with the help of the $-2\\ln\\mathcal{L(\\alpha_{\\mathrm{S}})}$ function which is to be implemented independently. The previously determined parameters $\\theta_{\\mathrm{S}}$ and $\\theta_{\\mathrm{U}}$ of the probability density function of the signal or background are fixed for the determination (profiled Likelihood).\n",
    "\n",
    "The evaluation of the function value $-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)$ should be emphasized here. This corresponds to the assumption that there is no signal component in the existing measurement and the measurement can therefore be explained by the null hypothesis (only background). In asymptotic form, therefore, the significance can be determined analogously to the expression in the chapter on parameterization of the signal MC expression with: $$ Z = \\sqrt{\\frac{-2\\ln\\mathcal{L}(\\alpha_{\\mathrm{S}} = 0)}{\\mathcal{L}_{\\mathrm{min}}}} \\, .$$\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "     \n",
    "Implement the $-2\\ln\\mathcal{L(\\alpha_S, m_{4\\ell})}$ function (if not already present).Determine the signal probability plus uncertainty of the measurement.  Specify the significance with which a higgs-like boson is measured. In addition, vary the mass of the higgs-like boson for the two-dimensional variant and compare it with the literature. Visualize and discuss your results.\n",
    "    \n",
    "For the minimization you can use `minuit` as in QRT. The numerical integration can again be done using `scipy.integrate.quad`.\n",
    "    \n",
    "It is also possible to perform an `UninnedFit` using `kafe2` and implement the extended likelihood as a cost function independently.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# measurement:\n",
    "from include.histogramm.HistDataGetter import HistDataGetter as HDG\n",
    "\n",
    "bins, hist_range, info = 999, (999.1, 999.2), [[\"2012\"], [\"B\", \"C\"]]\n",
    "mc_dir = \"../data/for_long_analysis/mc_aftH/\" \n",
    "ru_dir = \"../data/for_long_analysis/ru_aftH/\"\n",
    "raw_data = HDG(bins=bins, hist_range=hist_range, \n",
    "           info=info, mc_dir=mc_dir, ru_dir=ru_dir).get_data_raw(\"mass_4l\")\n",
    "raw_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code iminuit\n",
    "\n",
    "def extended_nll_iminuit(alpha_s, mass=125.0):\n",
    "    # code\n",
    "    return mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code kafe2\n",
    "\n",
    "def extended_nll_kafe2(data, model, parameter_values, parameter_constraints):\n",
    "    \"\"\"\n",
    "    \n",
    "    :param data: measurement array\n",
    "    :param model: model array (calculate individual in # code)\n",
    "    :param parameter_values:\n",
    "    :param parameter_constraints: (not used)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    # check if one of the parameter_values is nan\n",
    "    if any(f\"{par}\" == \"nan\" for par in parameter_values):\n",
    "        return np.inf\n",
    "    \n",
    "    # your code\n",
    "    \n",
    "    _total_log_likelihood = 0.0 # your code\n",
    "    \n",
    "    # check if _total_log_likelihood is nan\n",
    "    if np.isnan(_total_log_likelihood):\n",
    "        return np.inf\n",
    "    return -2.0 * _total_log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 1D profiled Likelihood\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The one-dimensional variant only varies the signal probability. The mass is held after the parameterization and is not varied anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code (iminuit minimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from include.stattest.PlotHelper import PlotHelper\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes, mark_inset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "alpha_ = np.linspace(0.0, 0.8, 1000)\n",
    "nll_ = extended_nll(alpha_s=alpha_, mass=125)\n",
    "\n",
    "# as example (to be replaced with actual nll array)\n",
    "nll_ = 150*(alpha_ - 0.4) ** 2\n",
    "\n",
    "z_ = np.sqrt(nll_[0])\n",
    "\n",
    "# code to further customize the plot\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "PlotHelper.plot_nll_scan_main_window(ax_obj_=ax, \n",
    "                                     x_=alpha_, \n",
    "                                     y_=nll_, \n",
    "                                     z_=z_)\n",
    "\n",
    "ax_in = inset_axes(ax, width='50%', height='40%', loc=\"upper right\")\n",
    "PlotHelper.plot_nll_scan_inside_window(ax_obj_=ax_in, \n",
    "                                       x_=alpha_, y_=nll_, \n",
    "                                       x_ticks_=PlotHelper.get_error_points_from_nll(x_=alpha_, nll_=nll_))\n",
    "mark_inset(ax, ax_in, loc1=2, loc2=4, fc=\"none\", ec=\"grey\", lw=0.5, alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the 'kafe2' variant, an 'uninned container' must be created and adjusted. For the 1D variant it is recommended to use the method `fix_parameter(par_name, value)`.\n",
    "The significance can be calculated using the implemented `extended_nll_kafe2` cost function. It is recommended that you use the `limit_parameter` to restrict the parameter `alpha` to a reasonable interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import kafe2 as K2\n",
    "\n",
    "my_cost_function = K2.UnbinnedCostFunction_UserDefined(extended_nll_kafe2)\n",
    "\n",
    "# more code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2D profiled Likelihood - Mass determination\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Finally, the mass is also determined. This is done by the two-dimensional variation - mass and signal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Iminuit variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similar to the 1D version, the best mass can be determined at the same time. The procedure is analogous to the 1D variant/QRT implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code:\n",
    "#     - minimization\n",
    "#     - plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### kafe2 variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The only difference to the 1D variant is that the mass is no longer fixed by `fix_parameter`, but is varied in a certain interval (`limit_parameter`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Combination of the measurements: CMS and ATLAS\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In order to obtain a better statistical significance, it is advantageous to include further measurements in the overall assessment. This can be done either by adding more data or by another experiment. Within the scope of this task the second option is performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For the ATLAS experiment, therefore, similar steps as above would have to be carried out, although many of the cuts show deviations. To shorten the work a bit, the final measurement is provided, as well as the histogram entries of the MC simulations.\n",
    "\n",
    "In addition, the parameters of the most suitable functions (linear combination of centered legend trepolynomials up to the third order for the background and the DSCB distribution for the signal) are provided (but it is also possible to check these results yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kafe2 as K2\n",
    "import os\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "atlas_data_dir_ = \"../data/for_long_analysis/atlas_data_and_mc_hist/\"\n",
    "atlas_files = [os.path.join(atlas_data_dir_, file) for file in os.listdir(atlas_data_dir_) if \".csv\" in file and \"atlas\" in file]\n",
    "pprint(atlas_files)\n",
    "hist_range_ = (106, 151)\n",
    "num_bins = 45\n",
    "\n",
    "data_A = np.loadtxt(atlas_files[2], delimiter=\",\")\n",
    "data_A = data[(data >= hist_range_[0]) & (data <= hist_range_[1])]\n",
    "\n",
    "mc_bac = np.loadtxt(atlas_files[1], delimiter=\",\").T\n",
    "mc_bac_data = mc_bac[0]\n",
    "mc_bac_error = mc_bac[1]\n",
    "\n",
    "mc_sig = np.loadtxt(atlas_files[0], delimiter=\",\").T\n",
    "mc_sig_data = mc_sig[0]\n",
    "mc_sig_error = mc_sig[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_CMS = data\n",
    "data_ATLAS = data_A\n",
    "\n",
    "background_parameter_CMS = background_parameter\n",
    "signal_parameter_CMS = signal_parameter\n",
    "\n",
    "background_parameter_ATLAS  = np.array([0.025050083639971264, \n",
    "                                        0.00018744120591615378, \n",
    "                                        -1.178181555551107e-05, \n",
    "                                        1.1872474265844408e-07])\n",
    "signal_parameter_ATLAS = np.array([2.1395802886491615, \n",
    "                                   124.19274632605674, \n",
    "                                   0.8324353801486924, \n",
    "                                   1.8034218165656613, \n",
    "                                   14.109501070824912, \n",
    "                                   19.9999999999894])\n",
    "bfu_ATLAS = llc_background.grade_3\n",
    "sfu_ATLAS = sf.DSCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "Combine the two measurements and determine the resulting mass of the Higgs boson, which is the signal probability and the significance of a measurement of the Higgs boson.\n",
    "\n",
    "You can reuse/modify the functions you have already implemented.\n",
    "    \n",
    "Note:\n",
    "    $$ \\mathcal{L_{\\mathrm{tot}}(\\alpha_S, m_{4\\ell})} = \\mathcal{L_{1}(\\alpha_S, m_{4\\ell})} \\mathcal{L_{2}(\\alpha_S, m_{4\\ell})} $$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# code (1D and 2D minimization)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
